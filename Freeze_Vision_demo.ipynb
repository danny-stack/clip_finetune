{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface\"\n",
    "\n",
    "if not os.path.exists(\"D:/huggingface\"):\n",
    "    os.makedirs(\"D:/huggingface\")\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = \"./indo-fashion-dataset/versions/15/\"\n",
    "json_path = os.path.join(base_dir, \"train_data.json\")\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    input_data = []\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        input_data.append(obj)\n",
    "        \n",
    "test_path = os.path.join(base_dir, \"test_data.json\")\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = []\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        test_data.append(obj)\n",
    "\n",
    "val_path = os.path.join(base_dir, \"val_data.json\")\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    val_data = []\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        val_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, data, image_dir, processor):\n",
    "        # Filter out the problem data\n",
    "        problem_ids = [\"55852.jpeg\", \"68567.jpeg\", \"78873.png\", \"71885.png\"]\n",
    "        self.data = [item for item in data if not any(pid in item[\"image_path\"] for pid in problem_ids)]\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.data[idx][\"image_path\"])\n",
    "\n",
    "        try:\n",
    "            image_path = os.path.join(self.image_dir, self.data[idx][\"image_path\"])\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            processed_image = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"][0]\n",
    "            processed_text = self.processor.tokenizer(\n",
    "                text=self.data[idx][\"product_title\"],\n",
    "                padding=\"max_length\",\n",
    "                max_length=77,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )[\"input_ids\"][0]\n",
    "            \n",
    "            return processed_image, processed_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\cs224N\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"./indo-fashion-dataset/versions/15/\"\n",
    "json_path = os.path.join(base_dir, \"train_data.json\")\n",
    "image_dir = os.path.join(base_dir, \"\")\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "dataset = CustomDataset(input_data, image_dir, processor)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_dataset = CustomDataset(val_data, image_dir, processor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, image_dir, processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def convert_models_to_fp32(model):\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.float()\n",
    "        if param.grad is not None:\n",
    "            param.grad.data = param.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_vision(model, freeze_text_encoder=False, freeze_vision_layers = 6):\n",
    "    if freeze_text_encoder:\n",
    "        for layer in model.text_model.encoder.layers[:2]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "    for name, param in model.vision_model.named_parameters():\n",
    "        if \"encoder.layers\" in name:  \n",
    "            layer_idx = int(name.split(\".\")[2])  \n",
    "            if layer_idx < freeze_vision_layers:  \n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_scale: Trainable\n",
      "text_model.embeddings.token_embedding.weight: Trainable\n",
      "text_model.embeddings.position_embedding.weight: Trainable\n",
      "text_model.encoder.layers.0.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.0.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.0.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.0.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.0.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.0.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.0.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.0.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.0.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.0.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.0.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.0.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.0.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.0.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.0.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.0.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.1.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.1.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.1.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.1.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.1.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.1.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.1.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.1.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.1.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.1.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.1.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.1.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.1.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.1.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.1.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.1.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.2.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.2.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.2.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.2.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.2.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.2.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.2.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.2.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.2.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.2.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.2.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.2.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.2.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.2.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.2.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.2.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.3.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.3.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.3.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.3.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.3.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.3.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.3.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.3.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.3.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.3.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.3.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.3.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.3.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.3.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.3.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.3.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.4.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.4.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.4.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.4.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.4.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.4.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.4.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.4.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.4.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.4.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.4.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.4.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.4.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.4.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.4.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.4.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.5.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.5.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.5.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.5.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.5.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.5.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.5.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.5.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.5.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.5.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.5.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.5.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.5.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.5.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.5.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.5.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.6.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.6.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.6.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.6.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.6.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.6.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.6.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.6.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.6.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.6.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.6.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.6.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.6.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.6.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.6.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.6.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.7.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.7.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.7.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.7.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.7.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.7.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.7.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.7.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.7.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.7.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.7.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.7.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.7.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.7.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.7.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.7.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.8.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.8.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.8.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.8.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.8.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.8.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.8.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.8.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.8.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.8.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.8.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.8.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.8.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.8.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.8.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.8.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.9.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.9.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.9.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.9.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.9.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.9.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.9.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.9.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.9.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.9.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.9.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.9.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.9.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.9.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.9.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.9.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.10.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.10.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.10.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.10.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.10.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.10.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.10.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.10.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.10.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.10.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.10.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.10.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.10.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.10.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.10.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.10.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.11.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.11.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.11.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.11.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.11.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.11.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.11.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.11.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.11.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.11.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.11.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.11.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.11.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.11.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.11.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.11.layer_norm2.bias: Trainable\n",
      "text_model.final_layer_norm.weight: Trainable\n",
      "text_model.final_layer_norm.bias: Trainable\n",
      "vision_model.embeddings.class_embedding: Trainable\n",
      "vision_model.embeddings.patch_embedding.weight: Trainable\n",
      "vision_model.embeddings.position_embedding.weight: Trainable\n",
      "vision_model.pre_layrnorm.weight: Trainable\n",
      "vision_model.pre_layrnorm.bias: Trainable\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.0.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.0.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.0.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.0.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.0.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.0.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.0.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.0.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.1.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.1.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.1.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.1.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.1.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.1.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.1.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.1.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.2.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.2.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.2.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.2.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.2.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.2.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.2.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.2.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.3.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.3.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.3.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.3.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.3.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.3.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.3.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.3.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.4.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.4.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.4.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.4.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.4.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.4.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.4.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.4.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.5.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.5.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.5.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.5.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.5.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.5.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.5.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.5.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.6.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.6.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.6.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.6.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.6.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.6.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.6.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.6.layer_norm2.bias: Trainable\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.7.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.7.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.7.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.7.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.7.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.7.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.7.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.7.layer_norm2.bias: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.8.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.8.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.8.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.8.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.8.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.8.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.8.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.8.layer_norm2.bias: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.9.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.9.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.9.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.9.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.9.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.9.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.9.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.9.layer_norm2.bias: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.10.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.10.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.10.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.10.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.10.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.10.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.10.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.10.layer_norm2.bias: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.11.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.11.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.11.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.11.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.11.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.11.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.11.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.11.layer_norm2.bias: Trainable\n",
      "vision_model.post_layernorm.weight: Trainable\n",
      "vision_model.post_layernorm.bias: Trainable\n",
      "visual_projection.weight: Trainable\n",
      "text_projection.weight: Trainable\n"
     ]
    }
   ],
   "source": [
    "model_freeze_vision = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_freeze_vision = model_freeze_vision.to(device)\n",
    "\n",
    "freeze_vision(model_freeze_vision, freeze_text_encoder=False, freeze_vision_layers = 6)\n",
    "\n",
    "for name, param in model_freeze_vision.named_parameters():\n",
    "    print(f\"{name}: {'Frozen' if not param.requires_grad else 'Trainable'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earlystopping import EarlyStopping\n",
    "from evaluate import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2849 [00:00<?, ?it/s]d:\\Anaconda3\\envs\\cs224N\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:480: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Epoch 1/15, Avg Loss: 0.7448: 100%|██████████| 2849/2849 [22:16<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 2.0478, Current LR: 0.000038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Avg Loss: 0.6993: 100%|██████████| 2849/2849 [21:57<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8497, Current LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Avg Loss: 0.5586: 100%|██████████| 2849/2849 [22:00<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.7303, Current LR: 0.000048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Avg Loss: 0.4737: 100%|██████████| 2849/2849 [24:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6637, Current LR: 0.000046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Avg Loss: 0.4024: 100%|██████████| 2849/2849 [22:03<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6780, Current LR: 0.000042\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Avg Loss: 0.3456: 100%|██████████| 2849/2849 [22:08<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6189, Current LR: 0.000038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Avg Loss: 0.2914: 100%|██████████| 2849/2849 [22:12<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5547, Current LR: 0.000032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Avg Loss: 0.2482: 100%|██████████| 2849/2849 [22:11<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4881, Current LR: 0.000027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Avg Loss: 0.2042: 100%|██████████| 2849/2849 [22:09<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4947, Current LR: 0.000021\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Avg Loss: 0.1712: 100%|██████████| 2849/2849 [22:10<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4738, Current LR: 0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Avg Loss: 0.1437: 100%|██████████| 2849/2849 [22:10<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5159, Current LR: 0.000010\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Avg Loss: 0.1205: 100%|██████████| 2849/2849 [22:23<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5045, Current LR: 0.000006\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Avg Loss: 0.1062: 100%|██████████| 2849/2849 [22:42<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5283, Current LR: 0.000003\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Avg Loss: 0.0964: 100%|██████████| 2849/2849 [22:09<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5348, Current LR: 0.000001\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Avg Loss: 0.0945: 100%|██████████| 2849/2849 [22:06<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5372, Current LR: 0.000000\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "# optimizer = torch.optim.Adam(model_freeze.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "optimizer = torch.optim.AdamW(model_freeze_vision.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.1)\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=7500,  \n",
    "#     gamma=0.75  \n",
    "# )\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-5,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,  \n",
    "    final_div_factor=10  \n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/clip_training')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_freeze_vision.train()\n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "    pbar = tqdm(train_loader,total=len(train_loader))\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, texts = batch\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        outputs = model_freeze_vision(input_ids=texts, pixel_values=images)\n",
    "        # ground_truth = torch.arange(len(images), device=device)\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        batch_loss = (loss_img(outputs.logits_per_image, ground_truth) + \n",
    "                     loss_txt(outputs.logits_per_text, ground_truth))/2\n",
    "        \n",
    "        batch_loss.backward()\n",
    "\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            convert_models_to_fp32(model_freeze_vision)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # current_lr = scheduler.get_last_lr()[0]\n",
    "        train_loss += batch_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        avg_loss = train_loss / num_batches        \n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "   \n",
    "    model_freeze_vision.eval()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, texts = [x.to(device) for x in batch]\n",
    "            outputs = model_freeze_vision(input_ids=texts, pixel_values=images)\n",
    "            ground_truth = torch.arange(len(images), device=device)\n",
    "            loss = (loss_img(outputs.logits_per_image, ground_truth) + \n",
    "                    loss_txt(outputs.logits_per_text, ground_truth))/2\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_avg_loss = val_loss / len(val_loader)\n",
    "    print(f\"Val Loss: {val_avg_loss:.4f}, Current LR: {current_lr:.6f}\")\n",
    "\n",
    "    early_stopping(val_avg_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    writer.add_scalars('Loss', {\n",
    "        'train': train_loss/len(train_loader),\n",
    "        'val': val_avg_loss,\n",
    "        'Learning Rate': current_lr\n",
    "    }, epoch)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "total_loss = avg_loss\n",
    "save_path = \"./checkpoints/clip_checkpoint_FreezeVision.pt\"\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model_freeze_vision.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 469/469 [00:33<00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "I2T_R@1: 0.1924\n",
      "T2I_R@1: 0.1873\n",
      "I2T_R@5: 0.4407\n",
      "T2I_R@5: 0.4397\n",
      "I2T_R@10: 0.5607\n",
      "T2I_R@10: 0.5613\n"
     ]
    }
   ],
   "source": [
    "# freeze the first six layers of the vision encoder\n",
    "\n",
    "metrics = evaluate_model(model_freeze_vision, test_loader, device)\n",
    "print(\"Test Results:\")\n",
    "for k, v in metrics.items():\n",
    "   print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 469/469 [00:33<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "I2T_R@1: 0.0351\n",
      "T2I_R@1: 0.0220\n",
      "I2T_R@5: 0.1085\n",
      "T2I_R@5: 0.0707\n",
      "I2T_R@10: 0.1557\n",
      "T2I_R@10: 0.1117\n"
     ]
    }
   ],
   "source": [
    "# zero-shot prediction\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = model.to(device)\n",
    "metrics = evaluate_model(model, test_loader, device)\n",
    "print(\"Test Results:\")\n",
    "for k, v in metrics.items():\n",
    "   print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 469/469 [00:33<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "I2T_R@1: 0.1652\n",
      "T2I_R@1: 0.1548\n",
      "I2T_R@5: 0.4007\n",
      "T2I_R@5: 0.3861\n",
      "I2T_R@10: 0.5171\n",
      "T2I_R@10: 0.5160\n"
     ]
    }
   ],
   "source": [
    "# Finetune all the layers\n",
    "\n",
    "unfreeze_path = \"./checkpoints/clip_checkpoint_unfreeze.pt\"\n",
    "unfreeze_checkpoint = torch.load(unfreeze_path)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.load_state_dict(unfreeze_checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "metrics = evaluate_model(model, test_loader, device)\n",
    "print(\"Test Results:\")\n",
    "for k, v in metrics.items():\n",
    "   print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 469/469 [00:34<00:00, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "I2T_R@1: 0.1659\n",
      "T2I_R@1: 0.1599\n",
      "I2T_R@5: 0.3944\n",
      "T2I_R@5: 0.3887\n",
      "I2T_R@10: 0.5117\n",
      "T2I_R@10: 0.5053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Freeze the backbone of both vision and text encoders\n",
    "\n",
    "Mixfreeze_path = \"./checkpoints/clip_checkpoint_MixFreeze.pt\"\n",
    "Mixfreeze_checkpoint = torch.load(Mixfreeze_path)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.load_state_dict(Mixfreeze_checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "metrics = evaluate_model(model, test_loader, device)\n",
    "print(\"Test Results:\")\n",
    "for k, v in metrics.items():\n",
    "   print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V2T_retrieval(model, processor, image_path, text_queries, k=5):\n",
    "    \"\"\"Retrieval text from image\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    image_features = model.get_image_features(inputs['pixel_values'].to(device))\n",
    "\n",
    "    all_text_features = []\n",
    "    for text_query in text_queries:\n",
    "        inputs = processor.tokenizer(text=text_query, return_tensors=\"pt\", truncation=True)\n",
    "        # text_features = model.get_text_features(inputs['pixel_values'].to(device))\n",
    "        text_features = model.get_text_features(**inputs.to(device))\n",
    "        all_text_features.append(text_features)\n",
    "\n",
    "    all_text_features = torch.cat(all_text_features)\n",
    "    similarities = image_features @ all_text_features.T\n",
    "    top_indices = similarities.topk(k).indices\n",
    "\n",
    "    return [text_queries[idx] for idx in top_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Women's Khadi Cotton Saree With Blouse Piece\",\n",
       " \"Women's Saree with Solid Piece\",\n",
       " 'A photo of a blouse saree.',\n",
       " 'Ilkal Resham Cotton Traditional Satin Border Tope Teni Pallu Saree',\n",
       " \"X SUNEET VARMA Women's Georgette and Pattern Mesh Ruffle Saree & Solid Blouse\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_query = \"Women's Digital Cotton Linen Blend Saree with Unstitched Blouse Piece(DigiPatta)\"\n",
    "# Women's Khadi Cotton Saree With Blouse Piece (UFO301119SOLD_KHDI_1_Grey)\n",
    "text_queries = [\n",
    "    \"Women's Georgette Saree with Blouse Piece\",\n",
    "    \"Women's Khadi Cotton Saree With Blouse Piece\",\n",
    "    \"Georgette Strip Print Saree\",\n",
    "    \"A photo of a blue saree.\",\n",
    "    \"X SUNEET VARMA Women's Georgette and Pattern Mesh Ruffle Saree & Solid Blouse\",\n",
    "    \"Women's Digital Cotton Linen Blend Saree with Unstitched Blouse Piece(DigiPatta)\",\n",
    "    \"A photo of a blouse saree.\",\n",
    "    \"Ilkal Resham Cotton Traditional Satin Border Tope Teni Pallu Saree\",\n",
    "    \"Women's Saree with Solid Piece\",\n",
    "    \"Women's Patola Style Art Silk Saree\"\n",
    "]\n",
    "image_path = \"./indo-fashion-dataset/versions/15/images/val/0.jpeg\"\n",
    "\n",
    "V2T_retrieval(model_freeze_vision, processor, image_path, text_queries, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T2V_retrieval(model, processor, text_query, image_folder,k=10):\n",
    "    \"\"\"Retrieval image from text\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    inputs = processor.tokenizer(text=text_query, return_tensors=\"pt\", truncation=True)\n",
    "    text_features = model.get_text_features(**inputs.to(device))\n",
    "\n",
    "    all_image_features = []\n",
    "    all_image_paths = []\n",
    "    for img_path in tqdm(glob.glob(f\"{image_folder}/*.jpg\")):\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(images=image, return_tensors=\"pt\")\n",
    "            image_features = model.get_image_features(inputs['pixel_values'].to(device))\n",
    "            all_image_features.append(image_features)\n",
    "            all_image_paths.append(img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    all_image_features = torch.cat(all_image_features)\n",
    "    similarities = text_features @ all_image_features.T\n",
    "    top_indices = similarities.topk(k).indices\n",
    "\n",
    "    return [all_image_paths[idx] for idx in top_indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"./indo-fashion-dataset/versions/15/images/val\"\n",
    "text_query = \"Women's Khadi Cotton Saree With Blouse Piece\"\n",
    "\n",
    "T2V_retrieval(model_freeze_vision, processor, text_query, image_folder, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224N",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
