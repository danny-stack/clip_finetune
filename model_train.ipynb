{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface\"\n",
    "\n",
    "if not os.path.exists(\"D:/huggingface\"):\n",
    "    os.makedirs(\"D:/huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/validmodel/indo-fashion-dataset?dataset_version_number=15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.69G/2.69G [02:34<00:00, 18.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\25472\\.cache\\kagglehub\\datasets\\validmodel\\indo-fashion-dataset\\versions\\15\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"validmodel/indo-fashion-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = \"./indo-fashion-dataset/versions/15/\"\n",
    "json_path = os.path.join(base_dir, \"train_data.json\")\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    input_data = []\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        input_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\cs224N\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, data, image_dir, processor):\n",
    "        # Filter out the problem data\n",
    "        problem_ids = [\"55852.jpeg\", \"68567.jpeg\", \"78873.png\", \"71885.png\"]\n",
    "        self.data = [item for item in data if not any(pid in item[\"image_path\"] for pid in problem_ids)]\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.data[idx][\"image_path\"])\n",
    "        try:\n",
    "            image_path = os.path.join(self.image_dir, self.data[idx][\"image_path\"])\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            processed_image = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"][0]\n",
    "            processed_text = self.processor.tokenizer(\n",
    "                text=self.data[idx][\"product_title\"],\n",
    "                padding=\"max_length\",\n",
    "                max_length=77,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )[\"input_ids\"][0]\n",
    "            \n",
    "            return processed_image, processed_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.data))\n",
    "        \n",
    "        #return processed[\"pixel_values\"][0], processed[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./indo-fashion-dataset/versions/15/\"\n",
    "json_path = os.path.join(base_dir, \"train_data.json\")\n",
    "image_dir = os.path.join(base_dir, \"\")\n",
    "\n",
    "dataset = CustomDataset(input_data, image_dir, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = \"./indo-fashion-dataset/versions/15/\"\n",
    "test_path = os.path.join(base_dir, \"test_data.json\")\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = []\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        test_data.append(obj)\n",
    "\n",
    "val_path = os.path.join(base_dir, \"val_data.json\")\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    val_data = []\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        val_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91162\n",
      "7500\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(test_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0: <class 'tuple'>\n",
      "Test 1: <class 'tuple'>\n",
      "Test 2: <class 'tuple'>\n",
      "Test 3: <class 'tuple'>\n",
      "Test 4: <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    sample = dataset[idx]\n",
    "    print(f\"Test {idx}:\", type(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing first sample loading...\n",
      "Single sample loaded successfully\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "Text shape: torch.Size([77])\n",
      "\n",
      "Testing batch loading...\n",
      "Batch loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "try:\n",
    "    print(\"Testing first sample loading...\")\n",
    "    single_sample = dataset[0]\n",
    "    print(\"Single sample loaded successfully\")\n",
    "    print(\"Image shape:\", single_sample[0].shape)\n",
    "    print(\"Text shape:\", single_sample[1].shape)\n",
    "except Exception as e:\n",
    "    print(\"Error loading single sample:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"\\nTesting batch loading...\")\n",
    "    batch = next(iter(dataloader))\n",
    "    print(\"Batch loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading batch:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_dataset = CustomDataset(val_data, image_dir, processor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, image_dir, processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Text batch shape: torch.Size([32, 77])\n"
     ]
    }
   ],
   "source": [
    "# 测试 DataLoader\n",
    "for images, texts in train_loader:\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Text batch shape:\", texts.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models_to_fp32(model):\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.float()\n",
    "        if param.grad is not None:\n",
    "            param.grad.data = param.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "   def __init__(self, patience=5, min_delta=0, verbose=True):\n",
    "       self.patience = patience\n",
    "       self.min_delta = min_delta\n",
    "       self.counter = 0\n",
    "       self.best_loss = None\n",
    "       self.early_stop = False\n",
    "       self.verbose = verbose\n",
    "\n",
    "   def __call__(self, val_loss):\n",
    "       if self.best_loss is None:\n",
    "           self.best_loss = val_loss\n",
    "       elif val_loss > self.best_loss - self.min_delta:\n",
    "           self.counter += 1\n",
    "           if self.verbose:\n",
    "               print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "           if self.counter >= self.patience:\n",
    "               self.early_stop = True\n",
    "       else:\n",
    "           self.best_loss = val_loss\n",
    "           self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Avg Loss: 0.9022: 100%|██████████| 2849/2849 [25:36<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8269, Current LR: 0.000028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Avg Loss: 0.4580: 100%|██████████| 2849/2849 [25:02<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5725, Current LR: 0.000012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Avg Loss: 0.2898: 100%|██████████| 2849/2849 [24:22<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4427, Current LR: 0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Avg Loss: 0.2038: 100%|██████████| 2849/2849 [24:30<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4406, Current LR: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Avg Loss: 0.1644: 100%|██████████| 2849/2849 [24:26<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4868, Current LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Avg Loss: 0.1671:   1%|          | 29/2849 [00:15<24:58,  1.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# current_lr = scheduler.get_last_lr()[0]\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     52\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     54\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m num_batches        \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.2)\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=1000,  \n",
    "    gamma=0.75 \n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/clip_training')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "    pbar = tqdm(train_loader,total=len(train_loader))\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, texts = batch\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=texts, pixel_values=images)\n",
    "        # ground_truth = torch.arange(len(images), device=device)\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        batch_loss = (loss_img(outputs.logits_per_image, ground_truth) + \n",
    "                     loss_txt(outputs.logits_per_text, ground_truth))/2\n",
    "        \n",
    "        batch_loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # current_lr = scheduler.get_last_lr()[0]\n",
    "        train_loss += batch_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        avg_loss = train_loss / num_batches        \n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "   \n",
    "    model.eval()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, texts = [x.to(device) for x in batch]\n",
    "            outputs = model(input_ids=texts, pixel_values=images)\n",
    "            ground_truth = torch.arange(len(images), device=device)\n",
    "            loss = (loss_img(outputs.logits_per_image, ground_truth) + \n",
    "                    loss_txt(outputs.logits_per_text, ground_truth))/2\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_avg_loss = val_loss / len(val_loader)\n",
    "    print(f\"Val Loss: {val_avg_loss:.4f}, Current LR: {current_lr:.6f}\")\n",
    "\n",
    "    early_stopping(val_avg_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    writer.add_scalars('Loss', {\n",
    "        'train': train_loss/len(train_loader),\n",
    "        'val': val_avg_loss,\n",
    "        'Learning Rate': current_lr\n",
    "    }, epoch)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "total_loss = avg_loss\n",
    "save_path = \"./checkpoints/clip_checkpoint.pt\"\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_data, image_dir, processor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Calculate image-text and text-image retrieval accuracy（R@1, R@5, R@10）\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_image_features = []\n",
    "    all_text_features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, texts in tqdm(test_loader, desc=\"Extracting features\"):\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "            \n",
    "            outputs = model(input_ids=texts, pixel_values=images)\n",
    "            image_features = outputs.image_embeds\n",
    "            text_features = outputs.text_embeds\n",
    "            \n",
    "            all_image_features.append(image_features)\n",
    "            all_text_features.append(text_features)\n",
    "    \n",
    "    image_features = torch.cat(all_image_features)\n",
    "    text_features = torch.cat(all_text_features)\n",
    "    \n",
    "    similarity = (image_features @ text_features.T)\n",
    "    \n",
    "    metrics = {}\n",
    "    for k in [1, 5, 10]:\n",
    "        i2t_recall = compute_recall_at_k(similarity, k)\n",
    "        t2i_recall = compute_recall_at_k(similarity.T, k)\n",
    "        metrics[f'I2T_R@{k}'] = i2t_recall\n",
    "        metrics[f'T2I_R@{k}'] = t2i_recall\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compute_recall_at_k(similarity, k):\n",
    "    batch_size = similarity.shape[0]\n",
    "    indices = torch.topk(similarity, k, dim=1)[1]\n",
    "    targets = torch.arange(batch_size).view(-1, 1).to(similarity.device)\n",
    "    correct = (indices == targets)\n",
    "    return (correct.sum(1) > 0).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 469/469 [00:45<00:00, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "I2T_R@1: 0.1652\n",
      "T2I_R@1: 0.1548\n",
      "I2T_R@5: 0.4007\n",
      "T2I_R@5: 0.3861\n",
      "I2T_R@10: 0.5171\n",
      "T2I_R@10: 0.5160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, test_loader, device)\n",
    "print(\"Test Results:\")\n",
    "for k, v in metrics.items():\n",
    "   print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_backbone(model, freeze_text_encoder=False):\n",
    "    \"\"\"\n",
    "    :param freeze_text_encoder: whether to freeze the text encoder\n",
    "    \"\"\"\n",
    "    if freeze_text_encoder:\n",
    "        # Freeze the first two layers of the text encoder\n",
    "        for layer in model.text_model.encoder.layers[:2]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "    # Freeze the first eight layers of the vision encoder\n",
    "    for name, param in model.vision_model.named_parameters():\n",
    "        if \"encoder.layers\" in name:  \n",
    "            layer_idx = int(name.split(\".\")[2])  \n",
    "            if layer_idx < 8:  \n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_scale: Trainable\n",
      "text_model.embeddings.token_embedding.weight: Trainable\n",
      "text_model.embeddings.position_embedding.weight: Trainable\n",
      "text_model.encoder.layers.0.self_attn.k_proj.weight: Frozen\n",
      "text_model.encoder.layers.0.self_attn.k_proj.bias: Frozen\n",
      "text_model.encoder.layers.0.self_attn.v_proj.weight: Frozen\n",
      "text_model.encoder.layers.0.self_attn.v_proj.bias: Frozen\n",
      "text_model.encoder.layers.0.self_attn.q_proj.weight: Frozen\n",
      "text_model.encoder.layers.0.self_attn.q_proj.bias: Frozen\n",
      "text_model.encoder.layers.0.self_attn.out_proj.weight: Frozen\n",
      "text_model.encoder.layers.0.self_attn.out_proj.bias: Frozen\n",
      "text_model.encoder.layers.0.layer_norm1.weight: Frozen\n",
      "text_model.encoder.layers.0.layer_norm1.bias: Frozen\n",
      "text_model.encoder.layers.0.mlp.fc1.weight: Frozen\n",
      "text_model.encoder.layers.0.mlp.fc1.bias: Frozen\n",
      "text_model.encoder.layers.0.mlp.fc2.weight: Frozen\n",
      "text_model.encoder.layers.0.mlp.fc2.bias: Frozen\n",
      "text_model.encoder.layers.0.layer_norm2.weight: Frozen\n",
      "text_model.encoder.layers.0.layer_norm2.bias: Frozen\n",
      "text_model.encoder.layers.1.self_attn.k_proj.weight: Frozen\n",
      "text_model.encoder.layers.1.self_attn.k_proj.bias: Frozen\n",
      "text_model.encoder.layers.1.self_attn.v_proj.weight: Frozen\n",
      "text_model.encoder.layers.1.self_attn.v_proj.bias: Frozen\n",
      "text_model.encoder.layers.1.self_attn.q_proj.weight: Frozen\n",
      "text_model.encoder.layers.1.self_attn.q_proj.bias: Frozen\n",
      "text_model.encoder.layers.1.self_attn.out_proj.weight: Frozen\n",
      "text_model.encoder.layers.1.self_attn.out_proj.bias: Frozen\n",
      "text_model.encoder.layers.1.layer_norm1.weight: Frozen\n",
      "text_model.encoder.layers.1.layer_norm1.bias: Frozen\n",
      "text_model.encoder.layers.1.mlp.fc1.weight: Frozen\n",
      "text_model.encoder.layers.1.mlp.fc1.bias: Frozen\n",
      "text_model.encoder.layers.1.mlp.fc2.weight: Frozen\n",
      "text_model.encoder.layers.1.mlp.fc2.bias: Frozen\n",
      "text_model.encoder.layers.1.layer_norm2.weight: Frozen\n",
      "text_model.encoder.layers.1.layer_norm2.bias: Frozen\n",
      "text_model.encoder.layers.2.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.2.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.2.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.2.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.2.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.2.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.2.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.2.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.2.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.2.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.2.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.2.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.2.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.2.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.2.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.2.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.3.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.3.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.3.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.3.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.3.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.3.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.3.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.3.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.3.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.3.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.3.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.3.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.3.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.3.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.3.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.3.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.4.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.4.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.4.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.4.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.4.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.4.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.4.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.4.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.4.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.4.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.4.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.4.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.4.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.4.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.4.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.4.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.5.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.5.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.5.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.5.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.5.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.5.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.5.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.5.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.5.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.5.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.5.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.5.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.5.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.5.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.5.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.5.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.6.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.6.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.6.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.6.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.6.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.6.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.6.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.6.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.6.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.6.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.6.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.6.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.6.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.6.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.6.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.6.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.7.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.7.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.7.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.7.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.7.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.7.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.7.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.7.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.7.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.7.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.7.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.7.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.7.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.7.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.7.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.7.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.8.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.8.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.8.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.8.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.8.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.8.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.8.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.8.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.8.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.8.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.8.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.8.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.8.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.8.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.8.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.8.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.9.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.9.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.9.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.9.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.9.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.9.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.9.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.9.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.9.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.9.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.9.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.9.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.9.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.9.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.9.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.9.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.10.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.10.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.10.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.10.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.10.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.10.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.10.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.10.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.10.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.10.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.10.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.10.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.10.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.10.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.10.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.10.layer_norm2.bias: Trainable\n",
      "text_model.encoder.layers.11.self_attn.k_proj.weight: Trainable\n",
      "text_model.encoder.layers.11.self_attn.k_proj.bias: Trainable\n",
      "text_model.encoder.layers.11.self_attn.v_proj.weight: Trainable\n",
      "text_model.encoder.layers.11.self_attn.v_proj.bias: Trainable\n",
      "text_model.encoder.layers.11.self_attn.q_proj.weight: Trainable\n",
      "text_model.encoder.layers.11.self_attn.q_proj.bias: Trainable\n",
      "text_model.encoder.layers.11.self_attn.out_proj.weight: Trainable\n",
      "text_model.encoder.layers.11.self_attn.out_proj.bias: Trainable\n",
      "text_model.encoder.layers.11.layer_norm1.weight: Trainable\n",
      "text_model.encoder.layers.11.layer_norm1.bias: Trainable\n",
      "text_model.encoder.layers.11.mlp.fc1.weight: Trainable\n",
      "text_model.encoder.layers.11.mlp.fc1.bias: Trainable\n",
      "text_model.encoder.layers.11.mlp.fc2.weight: Trainable\n",
      "text_model.encoder.layers.11.mlp.fc2.bias: Trainable\n",
      "text_model.encoder.layers.11.layer_norm2.weight: Trainable\n",
      "text_model.encoder.layers.11.layer_norm2.bias: Trainable\n",
      "text_model.final_layer_norm.weight: Trainable\n",
      "text_model.final_layer_norm.bias: Trainable\n",
      "vision_model.embeddings.class_embedding: Trainable\n",
      "vision_model.embeddings.patch_embedding.weight: Trainable\n",
      "vision_model.embeddings.position_embedding.weight: Trainable\n",
      "vision_model.pre_layrnorm.weight: Trainable\n",
      "vision_model.pre_layrnorm.bias: Trainable\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.0.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.0.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.0.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.0.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.0.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.0.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.0.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.0.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.1.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.1.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.1.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.1.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.1.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.1.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.1.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.1.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.2.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.2.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.2.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.2.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.2.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.2.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.2.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.2.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.3.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.3.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.3.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.3.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.3.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.3.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.3.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.3.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.4.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.4.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.4.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.4.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.4.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.4.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.4.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.4.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.5.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.5.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.5.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.5.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.5.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.5.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.5.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.5.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.6.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.6.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.6.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.6.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.6.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.6.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.6.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.6.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.weight: Frozen\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.bias: Frozen\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.weight: Frozen\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.bias: Frozen\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.weight: Frozen\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.bias: Frozen\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.weight: Frozen\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.bias: Frozen\n",
      "vision_model.encoder.layers.7.layer_norm1.weight: Frozen\n",
      "vision_model.encoder.layers.7.layer_norm1.bias: Frozen\n",
      "vision_model.encoder.layers.7.mlp.fc1.weight: Frozen\n",
      "vision_model.encoder.layers.7.mlp.fc1.bias: Frozen\n",
      "vision_model.encoder.layers.7.mlp.fc2.weight: Frozen\n",
      "vision_model.encoder.layers.7.mlp.fc2.bias: Frozen\n",
      "vision_model.encoder.layers.7.layer_norm2.weight: Frozen\n",
      "vision_model.encoder.layers.7.layer_norm2.bias: Frozen\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.8.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.8.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.8.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.8.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.8.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.8.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.8.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.8.layer_norm2.bias: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.9.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.9.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.9.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.9.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.9.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.9.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.9.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.9.layer_norm2.bias: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.10.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.10.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.10.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.10.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.10.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.10.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.10.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.10.layer_norm2.bias: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.weight: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.bias: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.weight: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.bias: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.weight: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.bias: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.weight: Trainable\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.bias: Trainable\n",
      "vision_model.encoder.layers.11.layer_norm1.weight: Trainable\n",
      "vision_model.encoder.layers.11.layer_norm1.bias: Trainable\n",
      "vision_model.encoder.layers.11.mlp.fc1.weight: Trainable\n",
      "vision_model.encoder.layers.11.mlp.fc1.bias: Trainable\n",
      "vision_model.encoder.layers.11.mlp.fc2.weight: Trainable\n",
      "vision_model.encoder.layers.11.mlp.fc2.bias: Trainable\n",
      "vision_model.encoder.layers.11.layer_norm2.weight: Trainable\n",
      "vision_model.encoder.layers.11.layer_norm2.bias: Trainable\n",
      "vision_model.post_layernorm.weight: Trainable\n",
      "vision_model.post_layernorm.bias: Trainable\n",
      "visual_projection.weight: Trainable\n",
      "text_projection.weight: Trainable\n"
     ]
    }
   ],
   "source": [
    "model_freeze = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_freeze = model_freeze.to(device)\n",
    "\n",
    "freeze_backbone(model_freeze, freeze_text_encoder=True)\n",
    "\n",
    "for name, param in model_freeze.named_parameters():\n",
    "    print(f\"{name}: {'Frozen' if not param.requires_grad else 'Trainable'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Avg Loss: 0.8894: 100%|██████████| 2849/2849 [21:30<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 2.0474, Current LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Avg Loss: 0.6199: 100%|██████████| 2849/2849 [22:19<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8842, Current LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Avg Loss: 0.5218: 100%|██████████| 2849/2849 [21:20<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.8388, Current LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Avg Loss: 0.4364: 100%|██████████| 2849/2849 [21:27<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6039, Current LR: 0.000038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Avg Loss: 0.3538: 100%|██████████| 2849/2849 [21:22<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6639, Current LR: 0.000038\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Avg Loss: 0.3252: 100%|██████████| 2849/2849 [21:18<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6248, Current LR: 0.000038\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Avg Loss: 0.2974: 100%|██████████| 2849/2849 [21:22<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6318, Current LR: 0.000038\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Avg Loss: 0.2463: 100%|██████████| 2849/2849 [21:24<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5952, Current LR: 0.000028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Avg Loss: 0.2241: 100%|██████████| 2849/2849 [21:25<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6175, Current LR: 0.000028\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Avg Loss: 0.2106: 100%|██████████| 2849/2849 [21:23<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6358, Current LR: 0.000028\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Avg Loss: 0.1946: 100%|██████████| 2849/2849 [21:26<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5876, Current LR: 0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Avg Loss: 0.1647: 100%|██████████| 2849/2849 [21:23<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6168, Current LR: 0.000021\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Avg Loss: 0.1575: 100%|██████████| 2849/2849 [21:30<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5993, Current LR: 0.000021\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Avg Loss: 0.1546: 100%|██████████| 2849/2849 [21:27<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6163, Current LR: 0.000021\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Avg Loss: 0.1338: 100%|██████████| 2849/2849 [21:25<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5699, Current LR: 0.000016\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "# optimizer = torch.optim.Adam(model_freeze.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "optimizer = torch.optim.AdamW(model_freeze.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.25)\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=10000, \n",
    "    gamma=0.75  \n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/clip_training')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_freeze.train()\n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "    pbar = tqdm(train_loader,total=len(train_loader))\n",
    "\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, texts = batch\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        outputs = model_freeze(input_ids=texts, pixel_values=images)\n",
    "        # ground_truth = torch.arange(len(images), device=device)\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        batch_loss = (loss_img(outputs.logits_per_image, ground_truth) + \n",
    "                     loss_txt(outputs.logits_per_text, ground_truth))/2\n",
    "        \n",
    "        batch_loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model_freeze.parameters(), max_norm=1.0)\n",
    "\n",
    "        # total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            convert_models_to_fp32(model_freeze)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # current_lr = scheduler.get_last_lr()[0]\n",
    "        train_loss += batch_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        avg_loss = train_loss / num_batches        \n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "   \n",
    "    model_freeze.eval()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, texts = [x.to(device) for x in batch]\n",
    "            outputs = model_freeze(input_ids=texts, pixel_values=images)\n",
    "            ground_truth = torch.arange(len(images), device=device)\n",
    "            loss = (loss_img(outputs.logits_per_image, ground_truth) + \n",
    "                    loss_txt(outputs.logits_per_text, ground_truth))/2\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_avg_loss = val_loss / len(val_loader)\n",
    "    print(f\"Val Loss: {val_avg_loss:.4f}, Current LR: {current_lr:.6f}\")\n",
    "\n",
    "    early_stopping(val_avg_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    writer.add_scalars('Loss', {\n",
    "        'train': train_loss/len(train_loader),\n",
    "        'val': val_avg_loss,\n",
    "        'Learning Rate': current_lr\n",
    "    }, epoch)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "total_loss = avg_loss\n",
    "save_path = \"./checkpoints/clip_checkpoint_MixFreeze.pt\"\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model_freeze.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss,\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 469/469 [00:35<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "I2T_R@1: 0.1659\n",
      "T2I_R@1: 0.1599\n",
      "I2T_R@5: 0.3944\n",
      "T2I_R@5: 0.3887\n",
      "I2T_R@10: 0.5117\n",
      "T2I_R@10: 0.5053\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model_freeze, test_loader, device)\n",
    "print(\"Test Results:\")\n",
    "for k, v in metrics.items():\n",
    "   print(f\"{k}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224N",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
